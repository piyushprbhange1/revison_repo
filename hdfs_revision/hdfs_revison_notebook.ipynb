{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of HDFS\n",
    "\n",
    "As part of this module we will be covering all important aspects of HDFS that are required for development. We have covered all the essentials for the development.\n",
    "* Using HDFS CLI\n",
    "* Getting Help or Usage\n",
    "* Listing HDFS Files\n",
    "* Managing HDFS Directories\n",
    "* Copying files from HDFS to Local\n",
    "* Copying files from HDFS to HDFS\n",
    "* Previewing data in HDFS Files\n",
    "* Getting File Metadata\n",
    "* HDFS Block Size\n",
    "* HDFS Replication Factor\n",
    "* Getting HDFS Storage Usage\n",
    "* Using HDFS Stat Commands\n",
    "* HDFS File Permissions\n",
    "* Overriding Properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HDFS CLI\n",
    "\n",
    "Let us understand how to use HDFS CLI to interact with HDFS.\n",
    "* Typically the cluster contain 3 types of nodes.\n",
    "  * Gateway nodes or client nodes or edge nodes\n",
    "  * Master nodes\n",
    "  * Worker nodes\n",
    "* Developers like us will typically have access to Gateway nodes or Client nodes.\n",
    "* We can connect to Gateway nodes or Client nodes using SSH.\n",
    "* Once login, we can interact with HDFS either by using `hadoop fs` or `hdfs dfs`. Both of them are aliases to each other.\n",
    "* `hadoop` have other subcommands than `fs` and is typically used to interact with HDFS or Map Reduce as developers.\n",
    "* `hdfs` have other subcommands than `dfs`. It is typically used to not only manage files in HDFS but also administrative tasks related HDFS components such as **Namenode**, **Secondary Namenode**, **Datanode** etc.\n",
    "* As deveopers, our scope will be limited to use `hdfs dfs` or `hadoop fs` to interact with HDFS.\n",
    "* Both have sub commands and each of the sub command take additional control arguments. Let us understand the structure by taking the example of `hdfs dfs -ls -l -S -r /public`.\n",
    "  * `hdfs` is the main command to manage all the components of HDFS.\n",
    "  * `dfs` is the sub command to manage files in HDFS.\n",
    "  * `-ls` is the file system command to list files in HDFS.\n",
    "  * `-l -S -r` are control arguments for `-ls` to control the run time behavior of the command.\n",
    "  * `/public` is the argument for the `-ls` command. It is path in HDFS. You will understad as you get into the details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting help or usage\n",
    "\n",
    "Let us explore details about how to list the commands and get the help or usage for given command.\n",
    "* Even though we can run commands from almost all the nodes in the clusters, we should only use Gateway to run HDFS Commands.\n",
    "* First we need to make sure designated Gateway server is Gateway for HDFS service so that we can run commands from Gateway node. In our case we have designated **gw02.itversity.com** or **gw03.itversity.com** as Gateways.\n",
    "* Typically Namenode process will be running on port number 8020. We can also pass namenode URI to access HDFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `hadoop fs` or `hdfs dfs` – list all the commands available\n",
    "* `hadoop fs -usage` – will give us basic usage for given command\n",
    "* `hadoop fs -help` – will give us additional information for all the commands. It is same as just running `hadoop fs` or `hdfs dfs`.\n",
    "* We can run help on individual commands as well - example: `hadoop fs -help ls` or `hdfs dfs -help ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "#  note : %sh to run code in bash enviorment  \n",
    "sudo mkdir -p /etc/hadoop\n",
    "sudo ln -s /opt/hadoop/etc/hadoop /etc/hadoop/conf\n",
    "head -20 /etc/hadoop/conf/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# note ${} used in bash to user variable here user is global enviorment variable\n",
    "hdfs dfs -ls /user/${USER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls hdfs://localhost:9000/user/${USER}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help\n",
    "hdfs dfs -usage ls\n",
    "hdfs dfs -help ls\n",
    "hdfs dfs -ls /public/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Listing HDFS Files\n",
    "\n",
    "Now let us walk through different options we have with hdfs `ls` command to list the files.\n",
    "* We can get usage by running `hdfs dfs -usage ls`.\n",
    "* We can get help using `hdfs dfs -help ls`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -usage ls\n",
    "hdfs dfs -help ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "!hdfs dfs -mkdir -p /public\n",
    "# note "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -put /data/nyse_all /public"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "#  sort  by reverse order \n",
    "hdfs dfs -ls -r /public/nyse_all/nyse_data\n",
    "# sort by time \n",
    "hdfs dfs -ls -t /public/nyse_all/nyse_data\n",
    "# sort by desc with directory \n",
    "hdfs dfs -ls -t -r /public/nyse_all/nyse_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can sort the files and directories by size using `-S`. By default, the files will be sorted in descending order by size. We can reverse the sorting order using `-S -r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -S /public/nyse_all/nyse_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h /public/nyse_all/nyse_data\n",
    "#  sorting in readable format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls -h -t /public/nyse_all/nyse_data\n",
    "#  sorting in time based size wise with time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managing HDFS Directories\n",
    "\n",
    "Now let us have a look at how to create directories and manage ownership.\n",
    "* By default hdfs is superuser of HDFS\n",
    "* `hadoop fs -mkdir` or `hdfs dfs -mkdir` – to create directories\n",
    "* `hadoop fs -chown` or `hdfs dfs -chown` – to change ownership of files\n",
    "* `chown` can also be used to change the group. We can change the group using `-chgrp` command as well. Make sure to run `-help` on chgrp and check the details.\n",
    "* Here are the steps to create user space. Only users in HDFS group can take care of it.\n",
    "  * Create directory with user id `itversity` under /user\n",
    "  * Change ownership to the same name as the directory created earlier (/user/itversity)\n",
    "  * You can validate permissions by using `hadoop fs -ls` or `hdfs dfs -ls` command on /user. Make sure to grep for the user name you are looking for.\n",
    "* Let's go ahead and create user space in HDFS for `itversity`. I have to login as sudoer and run below commands.\n",
    "\n",
    "```shell\n",
    "sudo -u hdfs hdfs dfs -mkdir /user/itversity\n",
    "sudo -u hdfs hdfs dfs -chown -R itversity:students /user/itversity\n",
    "hdfs dfs -ls /user|grep itversity\n",
    "```\n",
    "\n",
    "* You should be able to create folders under your home directory.\n",
    "* You can create the directory structure using `mkdir -p`. The existing folders will be ignored and non existing folders will be created.\n",
    "  * Let us run `hdfs dfs -mkdir -p /user/${USER}/retail_db/orders/year=2020`.\n",
    "  * As `/user/${USER}/retail_db` already exists, it will be ignored.\n",
    "  * Both `/user/${USER}/retail_db/orders` as well as `/user/${USER}/retail_db/orders/year=2020` will be created.\n",
    "\n",
    "  * We can delete non empty directory using `hdfs dfs -rm -R` and empty directory using `hdfs dfs -rmdir`. We will explore `hdfs dfs -rm` in detail later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -ls /user/`whoami`\n",
    "# userspace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -mkdir /user/`whoami`/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying files from local to HDFS\n",
    "\n",
    "We can copy files from local file system to HDFS either by using `copyFromLocal` or `put` command.\n",
    "* `hdfs dfs -copyFromLocal` or `hdfs dfs -put` – to copy files or directories from local filesystem into HDFS. We can also use `hadoop fs` in place of `hdfs dfs`.\n",
    "* However, we will not be able to update or fix data in files when they are in HDFS. If we have to fix any data, we have to move file to local file system, fix data and then copy back to HDFS.\n",
    "* Files will be divided into blocks and will be stored on Datanodes in distributed fashion based on block size and replication factor. We will get into the details later.\n",
    "\n",
    "![test](https://s3.amazonaws.com/kaizen.itversity.com/hadoop-overview/04HDFSAnatomyOfFileWrite.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# note skiptrash willpermantly delte the file\n",
    "\n",
    "hdfs dfs -rm -R -skipTrash /user/`whoami`/retail_db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying files from HDFS to Local\n",
    "\n",
    "We can copy files from HDFS to local file system either by using `copyToLocal` or `get` command.\n",
    "* `hdfs dfs -copyToLocal` or `hdfs dfs -get` – to copy files or directories from HDFS to local filesystem.\n",
    "* It will read all the blocks using index in sequence and construct the file in local file system.\n",
    "* If the target file or directory already exists in the local file system, `get` will fail saying **already exists**\n",
    "* We can also use patterns while using `get` command to get files from HDFS to local file system. Also, we can pass multiple files or folders in HDFS to `get` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -help copyToLocal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copying files from HDFS to HDFS\n",
    "\n",
    "Let us understand how to copy files with in HDFS (from one HDFS location to another HDFS location). \n",
    "\n",
    "* We can use `hdfs dfs -cp` command to copy files with in HDFS.\n",
    "* One need to have at least read permission on source folders or files and write permission on target folder for `cp` command to work as expected.\n",
    "* We can also use patterns while using `cp` command to copy files within HDFS. Also, we can pass multiple files or folders in HDFS to `cp` command."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previewing data in HDFS Files\n",
    "\n",
    "Let us see how we can preview the data in HDFS.\n",
    "* If we are dealing with files contain text data (files of text file format), we can preview contents of the files using different commands as `-tail`, `-cat` etc.\n",
    "* `-tail` can be used to preview last 1 KB of the file\n",
    "* `-cat` can be used to print the whole contents of the file on the screen. Be careful while using `-cat` as it will take a while for even medium sized files.\n",
    "* If you want to get first few lines from file you can redirect output of `hadoop fs -cat` or `hdfs dfs -cat` to Linux `more` command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting File Metadata\n",
    "\n",
    "Let us see how to get metadata for the  files stored in HDFS using `hdfs fsck` command. \n",
    "* We have files copied under HDFS location `/user/${USER}/retail_db`. We also have some sample large files copied under HDFS location `/public/randomtextwriter`. We can use `hdfs fsck` command.\n",
    "* We will first see how to get metadata of these files and then try to interpret it in subsequent topics.\n",
    "* HDFS stands for Hadoop Distributed File System. It means files are copied in distributed fashion.\n",
    "* Our cluster have master nodes and worker nodes, in this case the files will be physically copied in the worker nodes where data node process is running. We will cover this as part of the HDFS architecture.\n",
    "* Here are the details about worker nodes along with corresponding private ips.\n",
    "\n",
    "|Private ip|Full DNS|Short DNS|\n",
    "|---|---|---|\n",
    "|172.16.1.102|wn01.itversity.com|wn01|\n",
    "|172.16.1.103|wn02.itversity.com|wn02|\n",
    "|172.16.1.104|wn03.itversity.com|wn03|\n",
    "|172.16.1.107|wn04.itversity.com|wn04|\n",
    "|172.16.1.108|wn05.itversity.com|wn05|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# hdfs fsck -help\n",
    "#  hdfs fsck file system cheack gives all information about distributed file \n",
    "\n",
    "hdfs fsck /user/${USER}/retail_db -files\n",
    "#  gives all information about all file isn retaildb folder \n",
    " hdfs fsck /user/${USER}/retail_db -files -blocks\n",
    "# all information about blocks\n",
    "hdfs fsck /user/${USER}/retail_db -files -blocks -locations\n",
    "## which file at what location \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "#  gives all infromation about part file in human readable form \n",
    "hdfs dfs -ls -h /public/randomtextwriter/part-m-00000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs fsck /public/randomtextwriter/part-m-00000 -files -blocks -locations\n",
    "#  code to get all fsck via location "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Blocksize\n",
    "\n",
    "Let us get into details related to blocksize in HDFS.\n",
    "* HDFS stands for Hadoop Distributed File System.\n",
    "* It means the large files will be physically stored on multiple nodes in distributed fashion.\n",
    "* Let us review the `hdfs fsck` output of `/public/randomtextwriter/part-m-00000`. The file is approximately 1 GB in size and you will see 9 files.\n",
    "  * 8 files of size 128 MB\n",
    "  * 1 file of size 28 MB approximately\n",
    "* It means a file of size 1 GB 28 MB is stored in 9 blocks. It is due to the default block size which is 128 MB.\n",
    "\n",
    "* The default block size is 128 MB and it is set as part of hdfs-site.xml.\n",
    "* The property name is `dfs.blocksize`.\n",
    "* If the file size is smaller than default blocksize (128 MB), then there will be only one block as per the size of the file.\n",
    "* Let us determine the number of blocks for `/data/retail_db/orders/part-00000`. If we store this file of size 2.9 MB in HDFS, there will be one block associated with it as size of the file is less than the block size.\n",
    "* It occupies 2.9 MB storage in HDFS (assuming replication factor as 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs fsck /public/yelp-dataset-json/yelp_academic_dataset_user.json \\\n",
    "    -files \\\n",
    "    -blocks \\\n",
    "    -locations\n",
    "\n",
    "#  note this will validate the file size , location etc "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS Replication Factor\n",
    "\n",
    "Let us get an overview of replication factor - another important building block of HDFS.\n",
    "* While blocksize drives distribution of large files, replication factor drives reliability of the files.\n",
    "* If we only have one copy of each block for a given file and if the node goes down, then the data in the files is not readable.\n",
    "* HDFS replication mitigates this by maintaining multiple copies of each block.\n",
    "* Keep in mind that the default replication factor is **3** unless we override it.\n",
    "\n",
    "\n",
    "* As part of our lab cluster we maintain 2 copies of each block.\n",
    "* In production implementations, typically we have 3 copies with rack awareness enabled.\n",
    "* The default replication factor is 3 and it is set as part of hdfs-site.xml. In our case we have overridden to save the storage.\n",
    "* The property name is `dfs.replication`.\n",
    "* If the file size is smaller than default blocksize (128 MB), then there will be only one block as per the size of the file.\n",
    "* In a typical configuration with n replication factor, there will not be any down time even if n - 1 nodes go down in the cluster.\n",
    "* If replication factor is 3, cluster will be stable even if 2 of the nodes goes down in a cluster.\n",
    "* Replication factor covers all the hardware failures of the hosts.\n",
    "* In Production, we typically configure Rack Awareness which will get us much better reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "grep -B 1 -A 3 replication /etc/hadoop/conf/hdfs-site.xml\n",
    "#  to see default bash size property \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat %r /user/${USER}/retail_db/orders/part-00000\n",
    "# to get statestics hadoop part file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting HDFS Storage Usage\n",
    "\n",
    "Let us get an overview of HDFS usage using `du` and `df` commands.\n",
    "\n",
    "* We can use `hdfs dfs -df` to get the current capacity and usage of HDFS.\n",
    "* We can use `hdfs dfs -du` to get the size occupied by a file or folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "hdfs dfs -df\n",
    "#  gives filesystem capcity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%%sh\n",
    "\n",
    "hdfs dfs -du -s -h /user/${USER}/retail_db\n",
    "#  gives capcity of files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using HDFS Stat Commands\n",
    "\n",
    "Let us understand how to get details about HDFS files such as replication factor, block size etc.\n",
    "\n",
    "* `hdfs dfs -stat` can be used to get the statistics related to file or directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -stat /user/${USER}/retail_db/orders\n",
    "#  statestics of files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HDFS File Permissions\n",
    "\n",
    "Let us go through file permissions in HDFS.\n",
    "\n",
    "* As we create the files, we can check the permissions on them using `-ls` command.\n",
    "* Typically the owner of the user space will have **rwx**, while members of the group specified as well as others have **r-x**.\n",
    "* **rwx** stands for read, write and execute while **r-x** stands for only read and execute permissions.\n",
    "* We can change the permissions using `hadoop fs -chmod` or `hdfs dfs -chmod`. However one can change the permissions of their own files.\n",
    "* We can specify permissions mode (e.g.: `+x` to grant execute access to owner, group as well as others) as well as octal mode (e.g.: 755 to grant rwx for owner, rx for group and others)\n",
    "\n",
    "If you are not familiar with linux command chmod, we would highly recommend you to spend some time to get detailed understanding of it as it is very important with respect to file permissions.\n",
    "\n",
    "* Adding write permissions only to owner. Now the owner will be able to delete the file, but others cannot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "hdfs dfs -chmod -R -w /user/${USER}/retail_db/order_items\n",
    "hdfs dfs -chmod -R 757  /user/${USER}/retail_db/orders\n",
    "#  for changing permissions of hadoop file system "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overriding Properties\n",
    "\n",
    "Let us understand how we can override the properties while running `hdfs dfs` or `hadoop fs` commands.\n",
    "\n",
    "* We can change any property which is not defined as final in **core-site.xml** or **hdfs-site.xml**.\n",
    "* We can change `blocksize` as well as `replication` while copying the files. We can also change them after copying the files as well.\n",
    "* We can either pass individual properties using `-D` or bunch of properties by passing xml similar to **core-site.xml** or **hdfs-site.xml** as part of `--conf`.\n",
    "* Let's copy a file **/data/crime/csv/rows.csv** with default values. The file is splitted into 12 blocks with 2 copies each (as our default blocksize is 128 MB and replication factor is 2)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c833d68cfa606615f21364fbb488a2420df1f4f5f7a7d6ff6a76f9da5ef2719b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
