{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting Started\n",
    "## Setup Spark Locally - Ubuntu\n",
    "\n",
    "Let us setup Spark Locally on Ubuntu.\n",
    "\n",
    "* Install latest version of Anaconda\n",
    "* Make sure Jupyter Notebook is setup and validated.\n",
    "* Setup Spark and Validate.\n",
    "* Setup Environment Variables to integrate Pyspark with Jupyter Notebook.\n",
    "* Launch Jupyter Notebook using `pyspark` command.\n",
    "* Setup PyCharm (IDE) for application development.\n",
    "## Setup Spark Locally - Mac\n",
    "\n",
    "### Let us setup Spark Locally on Ubuntu.\n",
    "\n",
    "* Install latest version of Anaconda\n",
    "* Make sure Jupyter Notebook is setup and validated.\n",
    "* Setup Spark and Validate.\n",
    "* Setup Environment Variables to integrate Pyspark with Jupyter Notebook.\n",
    "* Launch Jupyter Notebook using `pyspark` command.\n",
    "* Setup PyCharm (IDE) for application development.\n",
    "\n",
    "## Signing up for ITVersity Labs\n",
    "\n",
    "Here are the steps for signing to ITVersity labs.\n",
    "* Go to https://labs.itversity.com\n",
    "* Sign up to our website\n",
    "* Purchase lab access\n",
    "* Go to lab page and create lab account\n",
    "* Login and practice\n",
    "\n",
    "## Using ITVersity Labs\n",
    "\n",
    "Let us understand how to submit the Spark Jobs in ITVersity Labs.\n",
    "\n",
    "* You can either use Jupyter based environment or `pyspark` in terminal to submit jobs in ITVersity labs.\n",
    "* You can also submit Spark jobs using `spark-submit` command.\n",
    "* As we are using Python we can also use the help command to get the documentation - for example `help(spark.read.csv)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting with File Systems\n",
    "\n",
    "Let us understand how to interact with file system using %fs command from Databricks Notebook.\n",
    "\n",
    "* We can access datasets using %fs magic command in Databricks notebook\n",
    "* By default, we will see files under dbfs\n",
    "* We can list the files using ls command - e. g.: `%fs ls`\n",
    "* Databricks provides lot of datasets for free under databricks-datasets\n",
    "* If the cluster is integrated with AWS or Azure Blob we can access files by specifying the appropriate protocol (e.g.: s3:// for s3)\n",
    "* List of commands available under `%fs`\n",
    "  * Copying files or directories `-cp`\n",
    "  * Moving files or directories `-mv`\n",
    "  * Creating directories `-mkdirs`\n",
    "  * Deleting files and directories `-rm`\n",
    "  * We can copy or delete directories recursively using `-r` or `--recursive`\n",
    "\n",
    "  ## Getting File Metadata\n",
    "\n",
    "Let us review the source location to get number of files and the size of the data we are going to process.\n",
    "\n",
    "* Location of airlines data dbfs:/databricks-datasets/airlines\n",
    "* We can get first 1000 files using %fs ls dbfs:/databricks-datasets/airlines\n",
    "* Location contain 1919 Files, however we will not be able to see all the details using %fs command.\n",
    "* Databricks File System commands does not have capability to understand metadata of files such as size in details.\n",
    "* When Spark Cluster is started, it will create 2 objects - spark and sc\n",
    "* sc is of type SparkContext and spark is of type SparkSession\n",
    "* Spark uses HDFS APIs to interact with the file system and we can access HDFS APIs using sc._jsc and sc._jvm to get file metadata.\n",
    "* Here are the steps to get the file metadata.\n",
    "  * Get Hadoop Configuration using `sc._jsc.hadoopConfiguration()` - let's say `conf`\n",
    "  * We can pass conf to `sc._jvm.org.apache.hadoop.fs.FileSystem` get to get FileSystem object - let's say `fs`\n",
    "  * We can build `path`  object by passing the path as string to `sc._jvm.org.apache.hadoop.fs.Path`\n",
    "  * We can invoke `listStatus` on top of fs by passing path which will return an array of FileStatus objects - let's say files.  \n",
    "  * Each `FileStatus` object have all the metadata of each file.\n",
    "  * We can use `len` on files to get number of files.\n",
    "  * We can use `>getLen` on each `FileStatus` object to get the size of each file. \n",
    "  * Cumulative size of all files can be achieved using `sum(map(lambda file: file.getLen(), files))`\n",
    "  \n",
    "Let us first get list of files \n",
    "\n",
    "note: dbfs databricks file system\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%fs ls dbfs:/databricks-datasets/airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the consolidated script to get number of files and cumulative size of all files in a given folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = sc._jsc.hadoopConfiguration()\n",
    "fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(conf)\n",
    "path = sc._jvm.org.apache.hadoop.fs.Path(\"dbfs:/databricks-datasets/airlines\")\n",
    "\n",
    "files = fs.listStatus(path)\n",
    "sum(map(lambda file: file.getLen(), files))/1024/1024/1024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platforms to Practice\n",
    "\n",
    "Let us understand different platforms we can leverage to practice Apache Spark using Python.\n",
    "\n",
    "* Local Setup\n",
    "* Databricks Platform\n",
    "* Setting up your own cluster\n",
    "* ITVersity Labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Spark Locally - Windows\n",
    "\n",
    "Let us understand how to setup Spark locally on Windows. Even though it can be setup directly, we would recommend to use virtual machine.\n",
    "\n",
    "* Here are the pre-requisites to setup Spark locally on Windows using Virtual Machine.\n",
    "* Make sure to setup Virtual Box and then Vagrant.\n",
    "\n",
    "\n",
    "## Setup Spark Locally - Mac\n",
    "\n",
    "Let us understand how to setup Spark locally on Mac.\n",
    "\n",
    "* Here are the pre-requisites to setup Spark Locally on mac.\n",
    "  * At least 8 GB RAM is highly desired.\n",
    "  * Make sure JDK 1.8 is setup\n",
    "  * Make sure to have Python 3. If you do not have it, you can install it using **homebrew**.\n",
    "* Here are the steps to setup Pyspark and validate.\n",
    "  * Create Python Virtual Environment - `python3 -m venv spark-venv`.\n",
    "  * Activate the virtual environment - `source spark-venv/bin/activate`.\n",
    "  * Run `pip install pyspark==2.4.6` to install Spark 2.4.6.\n",
    "  * Run `pyspark` to launch Spark CLI using Python as programming language.\n",
    "* Here are some of the limitations related to running Spark locally.\n",
    "  * You will be able to run Spark using local mode by default. But you will not be able to get the feel of Big Data.\n",
    "  * Actual production implementations will be on multinode cluters, which run using YARN or Spark Stand Alone or Mesos.\n",
    "  * You can understand the development process but you will not be able to explore best practices to build effective large scale data engineering solutions.\n",
    "\n",
    "  ## Setup Spark Locally - Ubuntu\n",
    "\n",
    "Let us understand how to setup Spark locally on Ubuntu.\n",
    "\n",
    "* Here are the pre-requisites to setup Spark Locally on Ubuntu.\n",
    "  * At least 8 GB RAM is highly desired.\n",
    "  * Make sure JDK 1.8 is setup\n",
    "  * Make sure to have Python 3. If you do not have it, you can install it using **apt** or **snap**.\n",
    "* Here are the steps to setup Pyspark and validate.\n",
    "  * Create Python Virtual Environment - `python3 -m venv spark-venv`.\n",
    "  * Activate the virtual environment - `source spark-venv/bin/activate`.\n",
    "  * Run `pip install pyspark==2.4.6` to install Spark 2.4.6.\n",
    "  * Run `pyspark` to launch Spark CLI using Python as programming language.\n",
    "* Here are some of the limitations related to running Spark locally.\n",
    "  * You will be able to run Spark using local mode by default. But you will not be able to get the feel of Big Data.\n",
    "  * Actual production implementations will be on multinode cluters, which run using YARN or Spark Stand Alone or Mesos.\n",
    "  * You can understand the development process but you will not be able to explore best practices to build effective large scale data engineering solutions.\n",
    "\n",
    "  ## Using ITVersity Labs\n",
    "\n",
    "Let me demonstrate how to use ITVersity Labs to practice Spark.\n",
    "* Once you sign up for the lab, you will get access to the cluster via Jupyter based environment.\n",
    "* You can connect to the labs using browser and practice in interactive fashion.\n",
    "* You can either use our material or upload your material to practice using Jupyter based environment.\n",
    "* Here are some of the advantages of using our labs.\n",
    "  * Interactive or Integrated learning experience.\n",
    "  * Access to multi node cluster.\n",
    "  * Pre-configured data sets as well as databases.\n",
    "  * You will be focused on the learning rather than troubleshooting the issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of File Systems\n",
    "\n",
    "Let us get an overview of File Systems you can work with while learning Spark.\n",
    "\n",
    "* Here are the file systems that can be used to learn Spark.\n",
    "  * Local file system when you run in local mode.\n",
    "  * Hadoop Distributed File System.\n",
    "  * AWS S3\n",
    "  * Azure Blob\n",
    "  * GCP Cloud Storage\n",
    "  * and other supported file systems.\n",
    "* It is quite straight forward to learn underlying file system. You just need to focus on the following:\n",
    "  * Copy files into the file system from different sources.\n",
    "  * Validate files in the file system.\n",
    "  * Ability to preview the data using Spark related APIs or direct tools.\n",
    "  * Delete files from the file system.\n",
    "* Typically we ingest data into underlying file system using tools such as Informatica, Talend, NiFi, Kafka, custom applications etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Spark Modules\n",
    "\n",
    "Let us understand details about different spark modules. We will be focusing on high level modules that are made available since Spark 2.2 and later.\n",
    "* Here are the different Spark Modules.\n",
    "  * Spark Core - RDD and Map Reduce APIs\n",
    "  * Spark Data Frames and Spark SQL\n",
    "  * Spark Structured Streaming\n",
    "  * Spark MLLib (Data Frame based)\n",
    "* As engineers, we need not focus too much on Spark Core libraries to build Data Pipelines. We should focus on Spark Data Frames as well as Spark SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster Manager Types\n",
    "\n",
    "Let us get an overview of different Spark Cluster Managers on which typically Spark Applications are deployed.\n",
    "\n",
    "* Here are the supported cluster manager types.\n",
    "  * Local (used for development and unit testing).\n",
    "  * Stand Alone\n",
    "  * YARN\n",
    "  * Mesos\n",
    "* Here are the popular distributions which use YARN to deploy Spark Applications.\n",
    "  * Cloudera\n",
    "  * AWS EMR\n",
    "  * Google Dataproc\n",
    "  * Hortonworks\n",
    "  * MapR\n",
    "* Databricks uses Stand Alone for running or deploying Spark Jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching Spark CLI\n",
    "\n",
    "Let us understand how to launch Pyspark CLI. We will be covering both local as well as our labs.\n",
    "* Once pyspark is installed you can run `pyspark` to launch Pyspark CLI.\n",
    "* In our labs, we have integrated Spark with Hadoop and Hive and you can interact with Hive Database as well.\n",
    "* You need to run the following command to launch Pyspark using Terminal.\n",
    "\n",
    "```shell\n",
    "export PYSPARK_PYTHON=python3\n",
    "export SPARK_MAJOR_VERSION=2\n",
    "pyspark --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "* Alternatively, you can also run the following command to launch Pyspark CLI.\n",
    "\n",
    "```shell\n",
    "pyspark --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "* Here is what happens when you launch Pyspark CLI.\n",
    "  * Launches Python CLI.\n",
    "  * All Spark related libraries will be loaded.\n",
    "  * Creates SparkSession as well as SparkContext objects.\n",
    "  * It facilitates us to explore Spark APIs in interactive fashion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Jupyter Lab Interface\n",
    "\n",
    "As part of our labs, you can learn Spark using Jupyter based interface.\n",
    "* Make sure you are using right kernel **Pyspark 2** (top right corner of the notebook).\n",
    "* Use below code to start the Spark Session object so that you can learn Spark in interactive fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "spark = SparkSession. \\\n",
    "    builder. \\\n",
    "    config('spark.ui.port', '0'). \\\n",
    "    config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "    enableHiveSupport(). \\\n",
    "    appName(f'{username} | Getting Started'). \\\n",
    "    master('yarn'). \\\n",
    "    getOrCreate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c833d68cfa606615f21364fbb488a2420df1f4f5f7a7d6ff6a76f9da5ef2719b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
