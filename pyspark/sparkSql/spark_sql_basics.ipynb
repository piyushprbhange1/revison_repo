{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Transformations\n",
    "\n",
    "As part of this section we will see basic transformations we can perform on top of Data Frames such as filtering, aggregations, joins etc using SQL. We will build end to end solution by taking a simple problem statement.\n",
    "\n",
    "* Spark SQL – Overview\n",
    "* Define Problem Statement\n",
    "* Preparing Tables\n",
    "* Projecting Data\n",
    "* Filtering Data\n",
    "* Joining Tables - Inner\n",
    "* Joining Tables - Outer\n",
    "* Perform Aggregations\n",
    "* Sorting Data\n",
    "* Conclusion - Final Solution\n",
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS.\n",
    "\n",
    "\n",
    "\n",
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val username = System.getProperty(\"user.name\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Basic Transformations\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL – Overview\n",
    "\n",
    "Let us get an overview of Spark SQL.\n",
    "\n",
    "Here are the standard operations which we typically perform as part of processing the data. In Spark we can perform these using Data Frame APIs or **Spark SQL**.\n",
    "\n",
    "\n",
    "* Selection or Projection – select clause\n",
    "  * It is also called as row level transformations.\n",
    "  * Apply standardization rules (convert names and addresses to upper case).\n",
    "  * Mask partial data (SSN and Date of births).\n",
    "* Filtering data – where clause\n",
    "  * Get orders based on date or product or category.\n",
    "* Joins – join (supports outer join as well)\n",
    "  * Join multiple data sets.\n",
    "* Aggregations – group by and aggregations with support of functions such as sum, avg, min, max etc\n",
    "  * Get revenue for a given order\n",
    "  * Get revenue for each order\n",
    "  * Get daily revenue\n",
    "* Sorting – order by\n",
    "  * Sort the final output by date.\n",
    "  * Sort the final output by date, then by revenue in descending order.\n",
    "  * Sort the final output by state or province, then by revenue in descending order.\n",
    "* Analytics Functions – aggregations, ranking and windowing functions\n",
    "  * Get top 5 stores by revenue for each state.\n",
    "  * Get top 5 products by revenue in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Problem Statement\n",
    "\n",
    "Let us define problemt statement to get an overview of basic transformations using Spark SQL.\n",
    "* Get Daily Product Revenue using orders and order_items data set.\n",
    "* We have following fields in **orders**.\n",
    "  * order_id\n",
    "  * order_date\n",
    "  * order_customer_id\n",
    "  * order_status\n",
    "* We have following fields in **order_items**.\n",
    "  * order_item_id\n",
    "  * order_item_order_id\n",
    "  * order_item_product_id\n",
    "  * order_item_quantity\n",
    "  * order_item_subtotal\n",
    "  * order_item_product_price\n",
    "* We have one to many relationship between orders and order_items.\n",
    "* **orders.order_id** is **primary key** and **order_items.order_item_order_id** is foreign key to **orders.order_id**.\n",
    "* By the end of this module we will explore all standard transformation and get daily product revenue using following fields.\n",
    "  * **orders.order_date**\n",
    "  * **order_items.order_item_product_id**\n",
    "  * **order_items.order_item_subtotal** (aggregated using date and product_id).\n",
    "* We will consider only **COMPLETE** or **CLOSED** orders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Tables\n",
    "\n",
    "Let us prepare the tables to solve the problem.\n",
    "\n",
    "* Make sure database is created.\n",
    "* Create **orders** table.\n",
    "* Load data from local path **/data/retail_db/orders** into newly created **orders** table.\n",
    "* Preview data and get count from **orders**\n",
    "* Create **order_items** table.\n",
    "* Load data from local path **/data/retail_db/order_items** into newly created **orders** table.\n",
    "* Preview data and get count from **order_items**\n",
    "\n",
    "As tables and data are ready let us get into how to write queries against tables to perform basic transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecting Data\n",
    "\n",
    "Let us understand different aspects of projecting data. We primarily using `SELECT` to project the data.\n",
    "\n",
    "* We can project all columns using `*` or some columns using column names.\n",
    "* We can provide aliases to a column or expression using `AS` in `SELECT` clause.\n",
    "* `DISTINCT` can be used to get the distinct records from selected columns. We can also use `DISTINCT *` to get unique records using all the columns.\n",
    "* As of now **Spark SQL** does not support projecting all but one or few columns. It is supported in Hive. Following will work in hive and it will project all the columns from orders except for order_id.\n",
    "\n",
    "```\n",
    "SET hive.support.quoted.identifiers=none;\n",
    "SELECT `(order_id)?+.+` FROM orders;\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val username = System.getProperty(\"user.name\")\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Basic Transformations\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first we are making table and other thing  with sql then with spark sql \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sql\n",
    "<!-- it will drop all database along with table  -->\n",
    "DROP DATABASE itversity_retail CASCADE;\n",
    "CREATE DATABASE IF NOT EXISTS itversity_retail;\n",
    "USE itversity_retail;\n",
    "CREATE TABLE orders (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "<!--  create order table hive  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/order_items\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/order_items' INTO TABLE order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/order_items\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM order_items LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(1) FROM order_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### same thing with spark sql "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"DROP DATABASE itversity_retail CASCADE\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS itversity_retail\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"USE itversity_retail\").show()\n",
    "spark.sql(\"SHOW tables\").show()\n",
    "spark.sql(\"DROP TABLE orders\").show()\n",
    "spark.sql(\"DROP TABLE orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "    order_id INT,\n",
    "    order_date STRING,\n",
    "    order_customer_id INT,\n",
    "    order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/orders\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/data/retail_db/orders' INTO TABLE orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/orders\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM orders LIMIT 10\").show()\n",
    "spark.sql(\"SELECT count(1) FROM orders\").show()\n",
    "spark.sql(\"DROP TABLE order_items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE order_items (\n",
    "    order_item_id INT,\n",
    "    order_item_order_id INT,\n",
    "    order_item_product_id INT,\n",
    "    order_item_quantity INT,\n",
    "    order_item_subtotal FLOAT,\n",
    "    order_item_product_price FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/order_items\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"LOAD DATA LOCAL INPATH '/data/retail_db/order_items' INTO TABLE order_items\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "val username = System.getProperty(\"user.name\")\n",
    "s\"hdfs dfs -ls /user/itversity/warehouse/${username}_retail.db/order_items\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT * FROM order_items LIMIT 10\").show()\n",
    "spark.sql(\"SELECT count(1) FROM order_items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "\n",
    "Let us understand how we can filter the data in Spark SQL.\n",
    "* We use `WHERE` clause to filter the data.\n",
    "* All comparison operators such as `=`, `!=`, `>`, `<`, etc can be used to compare a column or expression or literal with another column or expression or literal.\n",
    "* We can use operators such as LIKE with % and regexp_like for pattern matching.\n",
    "* Boolan OR and AND can be performed when we want to apply multiple conditions.\n",
    "  * Get all orders with order_status equals to COMPLETE or CLOSED. We can also use IN operator.\n",
    "  * Get all orders from month 2014 January with order_status equals to COMPLETE or CLOSED\n",
    "* We need to use `IS NULL` and `IS NOT NULL` to compare against null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE students (\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_number STRING,\n",
    "    student_address STRING\n",
    ") STORED AS avro\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO students \n",
    "VALUES (1, 'Scott', 'Tiger', NULL, NULL)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT * FROM students \n",
    "WHERE student_phone_number = NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_id,\n",
    "    o.order_date,\n",
    "    o.order_status,\n",
    "    oi.order_item_subtotal\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "    AND date_format(order_date, 'yyyy-MM') = '2014-01'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_id,\n",
    "    o.order_date,\n",
    "    o.order_status,\n",
    "    oi.order_item_order_id,\n",
    "    oi.order_item_subtotal\n",
    "FROM orders o LEFT OUTER JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE oi.order_item_order_id IS NULL\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "HAVING revenue >= 500\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can perform global aggregations as well as aggregations by key.\n",
    "* Global Aggregations\n",
    "  * Get total number of orders.\n",
    "  * Get revenue for a given order id.\n",
    "  * Get number of records with order_status either COMPLETED or CLOSED.\n",
    "* Aggregations by key - using `GROUP BY`\n",
    "  * Get number of orders by date or status.\n",
    "  * Get revenue for each order_id.\n",
    "  * Get daily product revenue (using order date and product id as keys).\n",
    "* We can also use `HAVING` clause to apply filtering on top of aggregated data.\n",
    "  * Get daily product revenue where revenue is greater than $500 (using order date and product id as keys).\n",
    "* Rules while using `GROUP BY`.\n",
    "  * We can have the columns which are specified as part of `GROUP BY` in `SELECT` clause.\n",
    "  * On top of those, we can have derived columns using aggregate functions.\n",
    "  * We cannot have any other columns that are not used as part of `GROUP BY` on derived column using non aggregate functions.\n",
    "  * We will not be able to use aggregate functions or aliases used in the select clause as part of the where clause.\n",
    "  * If we want to filter based on aggregated results, then we can leverage `HAVING` on top of `GROUP BY` (specifying `WHERE` is not an option)\n",
    "* Typical query execution - FROM -> WHERE -> GROUP BY -> SELECT\n",
    "\n",
    "* We typically perform sorting as final step.\n",
    "* Sorting can be done either by using one field or multiple fields.\n",
    "* We can sort the data either in ascending order or descending order by using column or expression.\n",
    "* By default, the sorting order is ascendig and we can change it to descending by using `DESC`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT o.order_date,\n",
    "    oi.order_item_product_id,\n",
    "    round(sum(oi.order_item_subtotal), 2) AS revenue\n",
    "FROM orders o JOIN order_items oi\n",
    "    ON o.order_id = oi.order_item_order_id\n",
    "WHERE o.order_status IN ('COMPLETE', 'CLOSED')\n",
    "GROUP BY o.order_date,\n",
    "    oi.order_item_product_id\n",
    "ORDER BY o.order_date,\n",
    "    revenue DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ## Exercises - Basic SQL Queries\n",
    "\n",
    "Here are some of the exercises for which you can write SQL queries to self evaluate.\n",
    "Let us start spark context for this Notebook so that we can execute the code provided. You can sign up for our [10 node state of the art cluster/labs](https://labs.itversity.com/plans) to learn Spark SQL using our unique integrated LMS.\n",
    "val username = System.getProperty(\"user.name\")\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Basic Transformations\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate\n",
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "\n",
    "```\n",
    "### Exercise 1 - Customer order count\n",
    "\n",
    "Get order count per customer for the month of 2014 January.\n",
    "* Tables - orders and customers\n",
    "* Data should be sorted in descending order by count and ascending order by customer id.\n",
    "* Output should contain customer_id, customer_first_name, customer_last_name and customer_order_count.\n",
    "### Exercise 2 - Dormant Customers\n",
    "\n",
    "Get the customer details who have not placed any order for the month of 2014 January.\n",
    "* Tables - orders and customers\n",
    "* Data should be sorted in ascending order by customer_id\n",
    "* Output should contain all the fields from customers\n",
    "### Exercise 3 - Revenue Per Customer\n",
    "\n",
    "Get the revenue generated by each customer for the month of 2014 January\n",
    "* Tables - orders, order_items and customers\n",
    "* Data should be sorted in descending order by revenue and then ascending order by customer_id\n",
    "* Output should contain customer_id, customer_first_name, customer_last_name, customer_revenue.\n",
    "* If there are no orders placed by customer, then the corresponding revenue for a give customer should be 0.\n",
    "* Consider only COMPLETE and CLOSED orders\n",
    "### Exercise 4 - Revenue Per Category\n",
    "\n",
    "Get the revenue generated for each category for the month of 2014 January\n",
    "* Tables - orders, order_items, products and categories\n",
    "* Data should be sorted in ascending order by category_id.\n",
    "* Output should contain all the fields from category along with the revenue as category_revenue.\n",
    "* Consider only COMPLETE and CLOSED orders\n",
    "### Exercise 5 - Product Count Per Department\n",
    "\n",
    "Get the products for each department.\n",
    "* Tables - departments, categories, products\n",
    "* Data should be sorted in ascending order by department_id\n",
    "* Output should contain all the fields from department and the product count as product_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic DDL and DML\n",
    "\n",
    "As part of this section we will primarily focus on basic DDL and DML using Spark Metastore.\n",
    "\n",
    "* Create Spark Metastore Tables\n",
    "* Overview of Data Types\n",
    "* Adding Comments\n",
    "* Loading Data into Tables - Local\n",
    "* Loading Data into Tables - HDFS\n",
    "* Loading Data - Append and Overwrite\n",
    "* Creating External Tables\n",
    "* Managed Tables vs. External Tables\n",
    "* Overview of File Formats\n",
    "* Dropping Tables and Databases\n",
    "* Truncating Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Spark Metastore Tables\n",
    "\n",
    "Let us understand how to create tables in Spark Metastore. We will be focusing on syntax and semantics.\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Managing Tables - Basic DDL and DML\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate\n",
    "If you are going to use CLIs, you can use Spark SQL using one of the 3 approaches.\n",
    "\n",
    "**Using Spark SQL**\n",
    "\n",
    "```\n",
    "spark2-sql \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Scala**\n",
    "\n",
    "```\n",
    "spark2-shell \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "\n",
    "**Using Pyspark**\n",
    "\n",
    "```\n",
    "pyspark2 \\\n",
    "    --master yarn \\\n",
    "    --conf spark.ui.port=0 \\\n",
    "    --conf spark.sql.warehouse.dir=/user/${USER}/warehouse\n",
    "```\n",
    "* Let us drop and recreate the table. We need to determine table type, file format based up on the files that will be copied to the table. If the file format is delimited text file then we need to understand field delimiter as well.\n",
    "  * Managed Table\n",
    "  * Text File Format\n",
    "  * Field Delimiter ','\n",
    "* We will create the table based on the structure of data in **/data/retail_db/orders**\n",
    "* If you are using `spark-sql` make sure to end the statements with **semi-colon**. With `spark-shell` or `pyspark` make sure to use `spark.sql` to pass these commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE students (\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_number STRING,\n",
    "    student_address STRING\n",
    ") STORED AS TEXTFILE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO students VALUES \n",
    "    (3, 'Mickey', 'Mouse', '2345678901', 'A Street, One City, Some State, 12345'),\n",
    "    (4, 'Bubble', 'Guppy', '6789012345', 'Bubbly Street, Guppy, La la land, 45678')\n",
    "\"\"\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "spark.sql(\"SHOW tables\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Syntactically Hive and Spark SQL are almost same.\n",
    "* Go to this [hive page](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL) and review supported data types.\n",
    "* Spark Metastore supports all standard data types.\n",
    "  * Numeric - INT, BIGINT, FLOAT etc\n",
    "  * Alpha Numeric or String - CHAR, VARCHAR, STRING\n",
    "  * Date and Timestamp - DATE, TIMESTAMP\n",
    "  * Special Data Types - ARRAY, STRUCT etc\n",
    "  * Boolean - BOOLEAN\n",
    "* If the file format is text file with special types, then we need to consider other clauses under DELIMITED ROW FORMAT (if we don't want to use default delimiters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE students (\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_numbers ARRAY<STRING>,\n",
    "    student_address STRUCT<street:STRING, city:STRING, state:STRING, zip:STRING>\n",
    ") STORED AS TEXTFILE\n",
    "ROW FORMAT\n",
    "    DELIMITED FIELDS TERMINATED BY '\\t'\n",
    "    COLLECTION ITEMS TERMINATED BY ','\n",
    "    MAP KEYS TERMINATED BY ':'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "INSERT INTO students VALUES \n",
    "    (3, 'Mickey', 'Mouse', ARRAY('1234567890', '2345678901'), STRUCT('A Street', 'One City', 'Some State', '12345')),\n",
    "    (4, 'Bubble', 'Guppy', ARRAY('5678901234', '6789012345'), STRUCT('Bubbly Street', 'Guppy', 'La la land', '45678'))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Comments\n",
    "\n",
    "Let us understand how to create table with comments in Hive using orders as example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "  order_id STRING COMMENT 'Unique order id',\n",
    "  order_date STRING COMMENT 'Date on which order is placed',\n",
    "  order_customer_id INT COMMENT 'Customer id who placed the order',\n",
    "  order_status STRING COMMENT 'Current status of the order'\n",
    ") COMMENT 'Table to save order level details'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Tables - Local\n",
    "\n",
    "Let us understand how to load data into Spark Metastore tables. We can load either from local file system or from HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "CREATE TABLE orders (\n",
    "  order_id INT COMMENT 'Unique order id',\n",
    "  order_date STRING COMMENT 'Date on which order is placed',\n",
    "  order_customer_id INT COMMENT 'Customer id who placed the order',\n",
    "  order_status STRING COMMENT 'Current status of the order'\n",
    ") COMMENT 'Table to save order level details'\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "\"\"\")\n",
    "spark.sql(\"LOAD DATA LOCAL INPATH '/data/retail_db/orders' INTO TABLE orders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into Tables - HDFS\n",
    "\n",
    "Let us understand how we can load data from HDFS location into Spark Metastore table.\n",
    "\n",
    "Note: spark sql and hive share metastore \n",
    "* We can use load command with-out **LOCAL** to get data from HDFS location into Spark Metastore Table.\n",
    "* User running load command from HDFS location need to have write permissions on the source location as data will be moved (deleted on source and copied to Spark Metastore table)\n",
    "* Make sure user have write permissions on the source location.\n",
    "* First we need to copy the data into HDFS location where user have write permissions.\n",
    "\n",
    "* If you look at **/user/training/retail_db** orders directory would have been deleted.\n",
    "* Move is much faster compared to copying the files by moving blocks around, hence Hive load command from HDFS location will always try to move files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s\"hadoop fs -put -f /data/retail_db/orders /user/${username}/retail_db\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "LOAD DATA INPATH '/user/itversity/retail_db/orders' \n",
    "  INTO TABLE orders\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data - Append and Overwrite\n",
    "Let us understand different approaches to load the data into Spark Metastore table.\n",
    "* `INTO TABLE` will append in the existing table\n",
    "* If we want to overwrite we have to specify `OVERWRITE INTO TABLE`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/orders' \n",
    "  INTO TABLE orders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/orders' \n",
    "  OVERWRITE INTO TABLE orders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating External Tables\n",
    "\n",
    "Let us understand how to create external table in Spark Metastore using orders as example. Also we will see how to load data into external table.\n",
    "\n",
    "* We just need to add **EXTERNAL** keyword in the **CREATE** clause and **LOCATION** after **STORED AS** clause or just **LOCATION** as part of **CREATE TABLE** statement.\n",
    "* We can use same LOAD commands to get data from either local file system or HDFS which we have used for Managed table.\n",
    "* Once table is created we can run `DESCRIBE FORMATTED orders` to check the metadata of the table and confirm whether it is managed table or external table.\n",
    "* We need to specify the location while creating external tables.\n",
    "\n",
    "Here is the script to create external table in Spark Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE EXTERNAL TABLE orders (\n",
    "  order_id INT COMMENT 'Unique order id',\n",
    "  order_date STRING COMMENT 'Date on which order is placed',\n",
    "  order_customer_id INT COMMENT 'Customer id who placed the order',\n",
    "  order_status STRING COMMENT 'Current status of the order'\n",
    ") COMMENT 'Table to save order level details'\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ','\n",
    "LOCATION '/user/itversity/external/retail_db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/orders' \n",
    "  INTO TABLE orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Managed Tables vs. External Tables\n",
    "\n",
    "Let us compare and contrast between Managed Tables and External Tables.\n",
    "\n",
    "* When we say EXTERNAL and specify LOCATION or LOCATION alone as part of CREATE TABLE, it makes the table EXTERNAL.\n",
    "* Rest of the syntax is same as Managed Table.\n",
    "* However, when we drop **Managed Table**, it will delete metadata from metastore as well as data from HDFS.\n",
    "* When we drop **External Table**, only metadata will be dropped, not the data.\n",
    "* Typically we use **External Table** when same dataset is processed by multiple frameworks such as Hive, Pig, Spark etc.\n",
    "* We cannot run **TRUNCATE TABLE** command against External Tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of File Formats\n",
    "Let us go through the details about different file formats supported by STORED AS Clause.\n",
    "\n",
    "* Go to this [page](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL) and review supported file formats.\n",
    "* Supported File Formats\n",
    "  * TEXTFILE\n",
    "  * ORC\n",
    "  * PARQUET\n",
    "  * AVRO\n",
    "  * SEQUENCEFILE - is not important\n",
    "  * JSONFILE - only available in recent vesions of Hive.\n",
    "  * and more\n",
    "* We can even specify custom file formats (out of scope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE students (\n",
    "    student_id INT,\n",
    "    student_first_name STRING,\n",
    "    student_last_name STRING,\n",
    "    student_phone_numbers ARRAY<STRING>,\n",
    "    student_address STRUCT<street:STRING, city:STRING, state:STRING, zip:STRING>\n",
    ") STORED AS parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Tables and Databases\n",
    "\n",
    "Let us understand how to DROP Spark Metastore Tables as well as Databases.\n",
    "* We can use **DROP TABLE** command to drop the table.. Let us drop orders table\n",
    "\n",
    "* **DROP TABLE** on managed table will delete both metadata in metastore as well as data in HDFS, while **DROP TABLE** on external table will only delete metadata in metastore.\n",
    "* We can drop database by using **DROP DATABASE** Command. However we need to drop all the tables in the database first.\n",
    "* Here is the example to drop the database itversity_retail - `DROP DATABASE itversity_retail`\n",
    "* We can also drop all the tables and databases by adding **CASCADE**.\n",
    "\n",
    "## Truncating Tables\n",
    "\n",
    "Let us understand how to truncate tables.\n",
    "* **TRUNCATE** works only for managed tables. Only data will be deleted, structure will be retained.\n",
    "* Launch Spark SQL\n",
    "\n",
    "## Managed Tables - Exercise\n",
    "Q:\n",
    "Let us use NYSE data and see how we can create tables in Spark Metastore.\n",
    "* Duration: **30 Minutes**\n",
    "* Data Location (Local): /data/nyse_all/nyse_data\n",
    "* Create a database with the name - YOUR_OS_USER_NAME_nyse\n",
    "* Table Name: nyse_eod\n",
    "* File Format: TEXTFILE (default)\n",
    "* Review the files by running Linux commands before using data sets. Data is compressed and we can load the files as is.\n",
    "* Copy one of the zip file to your home directory and preview the data. There should be 7 fields. You need to determine the delimiter.\n",
    "* Field Names: stockticker, tradedate, openprice, highprice, lowprice, closeprice, volume. For example, you need to use `BIGINT` for volume not `INT`.\n",
    "* Determine correct data types based on the values\n",
    "* Create Managed table with default Delimiter.\n",
    "> As delimiters in data and table are not same, you need to figure out how to get data into the target table.\n",
    "* Make sure the data is copied into the table as per the structure defined and validate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DML and Partitioning\n",
    "\n",
    "As part of this section we will continue understanding further concepts related to DML and also get into the details related to partitioning tables. With respect to DML, earlier we have seen how to use LOAD command, now we will see how to use INSERT command primarily to get query results copied into a table.\n",
    "\n",
    "* Introduction to Partitioning\n",
    "* Creating Tables using Parquet\n",
    "* LOAD vs. INSERT\n",
    "* Inserting Data using Stage Table\n",
    "* Creating Partitioned Tables\n",
    "* Adding Partitions to Tables\n",
    "* Loading data into Partitions\n",
    "* Inserting Data into Partitions\n",
    "* Using Dynamic Partition Mode\n",
    "* Exercise - Partitioned Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "val username = System.getProperty(\"user.name\")\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val username = System.getProperty(\"user.name\")\n",
    "val spark = SparkSession.\n",
    "    builder.\n",
    "    config(\"spark.ui.port\", \"0\").\n",
    "    config(\"spark.sql.warehouse.dir\", s\"/user/${username}/warehouse\").\n",
    "    enableHiveSupport.\n",
    "    appName(s\"${username} | Spark SQL - Managing Tables - DML and Partitioning\").\n",
    "    master(\"yarn\").\n",
    "    getOrCreate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Unlike Hive, Spark SQL does not support Bucketing which is similar to Hash Partitioning. However, Delta Lake does. Delta Lake is 3rd party library which facilitate us additional capabilities such as ACID transactions on top of Spark Metastore tables**\n",
    "\n",
    "Let us make sure that we have orders table with data as we will be using it to populate partitioned tables very soon.\n",
    "\n",
    "## Introduction to Partitioning\n",
    "\n",
    "Let us get an overview of partitioning of Spark Metastore tables.\n",
    "* It is similar to list partitioning where each partition is equal to a particular value for a given column.\n",
    "* Spark Metastore does not support range partitioning and bucketing. Bucketing is supported in Hive which is similar to Hash Partitioning.\n",
    "* Once the table is created, we can add static partitions and then load or insert data into it.\n",
    "* Spark Metastore also support creation of partitions dynamically, where partitions will be created based up on the partition column value.\n",
    "* A Partitioned table can be managed or external."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Tables using Parquet\n",
    "\n",
    "Let us create order_items table using Parquet file format. By default, the files of table using Parquet file format are compressed using Snappy algorithm.\n",
    "* A table with parquet file format can be external.\n",
    "* In our case we will create managed table with file format as parquet in STORED AS clause.\n",
    "* We will explore INSERT to insert query results into this table of type parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") STORED AS parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD vs. INSERT\n",
    "\n",
    "Let us compare and contrast LOAD and INSERT commands. These are the main approaches using which we get data into Spark Metastore tables.\n",
    "* LOAD will copy the files by dividing them into blocks.\n",
    "* LOAD is the fastest way of getting data into Spark Metastore tables. However, there will be minimal validations at File level. \n",
    "* There will be no transformations or validations at data level.\n",
    "* If it require any transformation while getting data into Spark Metastore table, then we need to use INSERT command.\n",
    "* Here are some of the usage scenarios of insert:\n",
    "  * Changing delimiters in case of text file format\n",
    "  * Changing file format\n",
    "  * Loading data into partitioned or bucketed tables (if bucketing is supported).\n",
    "  * Apply any other transformations at data level (widely used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example of load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE order_items (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") STORED AS parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/order_items'\n",
    "    INTO TABLE order_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_retail.db/order_items\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Data using Stage Table\n",
    "\n",
    "Let us understand how to insert data into order_items with Parquet file format. \n",
    "As data is in text file format and our table is created with Parquet file format, we will not be able to use LOAD command to load the data.\n",
    "Note : when format is issue we create stagging table the insert data from stage table to our original table \n",
    "* `INSERT INTO` will append data into the target table by adding new files.\n",
    "* `INSERT OVERWRITE` will overwrite the data in target table by deleting the files related to old data from the directory pointed by the Spark Metastore table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE order_items_stage (\n",
    "  order_item_id INT,\n",
    "  order_item_order_id INT,\n",
    "  order_item_product_id INT,\n",
    "  order_item_quantity INT,\n",
    "  order_item_subtotal FLOAT,\n",
    "  order_item_product_price FLOAT\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY ','"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/retail_db/order_items' INTO TABLE order_items_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO TABLE order_items\n",
    "SELECT * FROM order_items_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT OVERWRITE TABLE order_items\n",
    "SELECT * FROM order_items_stage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Partitioned Tables\n",
    "\n",
    "Let us understand how to create partitioned table and get data into that table.\n",
    "* Earlier we have already created orders table. We will use that as reference and create partitioned table.\n",
    "* We can use `PARTITIONED BY` clause to define the **column along with data type**. In our case we will use **order_month as partition column**.\n",
    "* We will not be able to directly load the data into the partitioned table using our original orders data (as data is not in sync with structure).\n",
    "\n",
    "Here is the example of creating partitioned tables in Spark Metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS orders_part;\n",
    "CREATE TABLE orders_part (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") PARTITIONED BY (order_month INT)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "DESCRIBE orders_part;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Partitions to Tables\n",
    "\n",
    "Let us understand how we can add static partitions to Partitioned tables in Spark Metastore.\n",
    "* We can add partitions using `ALTER TABLE` command with `ADD PARTITION`.\n",
    "* For each and every partition created, a subdirectory will be created using partition column name and corresponding value under the table directory.\n",
    "* Let us understand how to add partitions to **orders_part** table under **itversity_retail** database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DROP TABLE IF EXISTS orders_part;\n",
    "CREATE TABLE orders_part (\n",
    "  order_id INT,\n",
    "  order_date STRING,\n",
    "  order_customer_id INT,\n",
    "  order_status STRING\n",
    ") PARTITIONED BY (order_month STRING)\n",
    "ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE orders_part ADD\n",
    "    PARTITION (order_month=201308)\n",
    "    PARTITION (order_month=201309)\n",
    "    PARTITION (order_month=201310)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see patitions are now there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading into Partitions\n",
    "\n",
    "Let us understand how to use load command to load data into partitioned tables.\n",
    "\n",
    "* We need to make sure that file format of the file which is being loaded into table is same as the file format used while creating the table.\n",
    "* We also need to make sure that delimiters are consistent between files and table for text file format.\n",
    "* Also data should match the criteria for the partition into which data is loaded.\n",
    "* Our `/data/retail_db/orders` have data for the whole year and hence we should not load the data directly into partition.\n",
    "* We need to split into files matching partition criteria and then load into the table.\n",
    "\n",
    "To use load command to load the files into partitions we need to pre-partition the data based on partition logic. \n",
    "\n",
    "Here is the example of using simple shell commands to partition the data. Use command prompt to run these commands\n",
    "\n",
    "\n",
    "```shell\n",
    "rm -rf ~/orders\n",
    "mkdir -p ~/orders\n",
    "\n",
    "grep 2013-07 /data/retail_db/orders/part-00000 > ~/orders/orders_201307\n",
    "grep 2013-08 /data/retail_db/orders/part-00000 > ~/orders/orders_201308\n",
    "grep 2013-09 /data/retail_db/orders/part-00000 > ~/orders/orders_201309\n",
    "grep 2013-10 /data/retail_db/orders/part-00000 > ~/orders/orders_201310\n",
    "```\n",
    "\n",
    "Let us see how we can load data into corresponding partitions. Data has to be pre-partitioned based on the partitioned column.\n",
    "\n",
    "Note: adding  data on partions  basis via sql,spark sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/home/itversity/orders/orders_201307'\n",
    "  INTO TABLE orders_part PARTITION (order_month=201307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls -R /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inserting Data into Partitions\n",
    "\n",
    "Let us understand how to use insert to get data into static partitions in Spark Metastore from existing table called as orders.\n",
    "* Let us recap what is covered so far related to partitioned tables.\n",
    "  * We have created a table called as orders_part with order_month of type INT as partitioned column.\n",
    "  * We have added 4 static partitions for 201307, 201308, 201309 and 201310 using ALTER TABLE command.\n",
    "  * Once the table is created and partitions are added we have pre-processed the data to get data into the partitions using LOAD command.\n",
    "* It is not practical to use LOAD command always. We typically use `INSERT` via stage table to copy data into partitioned table.\n",
    "* We can pre-create partitions in partitioned tables and insert data into partitions using appropriate `INSERT `command. One need to ensure that required filter condition is applied to get the data relevant to the partition that is being populated.\n",
    "* We can also create partitions dynamically which we will see as part of the next topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "ALTER TABLE orders_part ADD PARTITION (order_month=201311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO TABLE orders_part PARTITION (order_month=201311)\n",
    "  SELECT * FROM orders WHERE order_date LIKE '2013-11%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls -R /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Dynamic Partition Mode\n",
    "\n",
    "Note: its important \n",
    "Let us understand how we can insert data into partitioned table using dynamic partition mode.\n",
    "* Using dynamic partition mode we need not pre create the partitions. Partitions will be automatically created when we issue INSERT command in dynamic partition mode.\n",
    "* To insert data using dynamic partition mode, we need to set the property `hive.exec.dynamic.partition` to **true**\n",
    "* Also we need to set `hive.exec.dynamic.partition.mode` to **nonstrict**\n",
    "\n",
    "Here is the example of inserting data into partitions using dynamic partition mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode;\n",
    "\n",
    "SET hive.exec.dynamic.partition.mode=nonstrict;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "INSERT INTO TABLE orders_part PARTITION (order_month)\n",
    "SELECT o.*, date_format(order_date, 'yyyyMM') order_month\n",
    "FROM orders o\n",
    "WHERE order_date >= '2013-12-01 00:00:00.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "s\"hdfs dfs -ls -R /user/${username}/warehouse/${username}_retail.db/orders_part\" !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise - Partitioned Tables\n",
    "\n",
    "Let us take care of this exercise related to partitioning to self evaluate our comfort level in working with partitioned tables.\n",
    "* Duration: **30 Minutes**\n",
    "* Use data from **/data/nyse_all/nyse_data**\n",
    "* Use database **YOUR_OS_USER_NAME_nyse**\n",
    "* Create partitioned table **nyse_eod_part**\n",
    "* Field Names: stockticker, tradedate, openprice, highprice, lowprice, closeprice, volume\n",
    "* Determine correct data types based on the values\n",
    "* Create Managed table with \",\" as delimiter.\n",
    "* Partition Field should be **tradeyear** and of type **INT** (one partition for corresponding year)\n",
    "* Insert data into partitioned table using dynamic partition mode.\n",
    "* Here are the steps to come up with the solution.\n",
    "  * Review the files under **/data/nyse_all/nyse_data** - determine data types (For example: tradedate should be INT and volume should be BIGINT)\n",
    "  * Create database **YOUR_OS_USER_NAME_nyse** (if it does not exists)\n",
    "  * Create non partitioned stage table\n",
    "  * Load data into non partitioned stage table\n",
    "  * Validate the count and also see that data is as expected by running simple select query.\n",
    "  * Create partitioned table\n",
    "  * Set required properties to use dynamic partition\n",
    "  * Insert data into partitioned table - here is how you can compute year from tradedate of type int `year(to_date(cast(tradedate AS STRING), 'yyyyMMdd')) AS tradeyear`\n",
    "  * Run below validate commands to validate\n",
    "\n",
    "  ### Validation\n",
    "Here are the instructions to validate the results.\n",
    "* Run `hdfs dfs -ls /user/YOUR_OS_USER_NAME/warehouse/YOUR_OS_USER_NAME_nyse.db/nyse_eod_part`\n",
    "* Run `SHOW PARTITIONS YOUR_OS_USER_NAME_nyse.nyse_eod_part`. You should see partitions for all the years using which you have loaded the data.\n",
    "* Run `SELECT count(1) FROM YOUR_OS_USER_NAME_nyse.nyse_eod_part`. The count should match the number of records in our dataset.\n",
    "* You can compare with the output generated by this simple Python code which is validated in our labs.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "path = r'/data/nyse_all/nyse_data' # use your path\n",
    "all_files = glob.glob(path + \"/*.txt.gz\")\n",
    "\n",
    "li = []\n",
    "\n",
    "for filename in all_files:\n",
    "    df = pd.read_csv(filename, index_col=None, header=None)\n",
    "    li.append(df)\n",
    "\n",
    "frame = pd.concat(li, axis=0, ignore_index=True)\n",
    "frame.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# second Basics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predefined Functions\n",
    "\n",
    "Let us go through the functions that can be used while processing the data. These are typically applied on columns to get derived values from existing column values.\n",
    "\n",
    "* Overview of Functions\n",
    "* Validating Functions\n",
    "* String Manipulation Functions\n",
    "* Date Manipulation Functions\n",
    "* Overview of Numeric Functions\n",
    "* Data Type Conversion\n",
    "* Handling NULL Values\n",
    "* Using CASE and WHEN\n",
    "* Query Example - Word Count\n",
    "\n",
    "## Overview of Functions\n",
    "Let us get overview of pre-defined functions in Spark SQL.\n",
    "* We can get list of functions by running `SHOW functions`\n",
    "* We can use DESCRIBE command to get the syntax and symantecs of a function - `DESCRIBE FUNCTION substr`\n",
    "* Following are the categories of functions that are more commonly used.\n",
    "  * String Manipulation\n",
    "  * Date Manipulation\n",
    "  * Numeric Functions\n",
    "  * Type Conversion Functions\n",
    "  * CASE and WHEN\n",
    "  * and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SHOW functions\").show(300, false)\n",
    "spark.catalog.listFunctions.show(300, false)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "DESCRIBE FUNCTION substr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating Functions\n",
    "Let us see how we can validate Spark SQL functions.\n",
    "\n",
    "* Spark SQL follows MySQL style. To validate functions we can just use SELECT clause - e. g.: `SELECT current_date;`\n",
    "* Another example - `SELECT substr('Hello World', 1, 5);`\n",
    "* If you want to use Oracle style, you can create table by name dual and insert one record.\n",
    "* You can also create temporary view on top of dataframe and start writing SQL Queries. We will see an example with Scala based approach. Here are the code snippets using both Scala as well as Pyspark.\n",
    "\n",
    "**Using Scala**\n",
    "```\n",
    "val orders = spark.read.\n",
    "    schema(\"order_id INT, order_date STRING, order_customer_id INT, order_status STRING\").\n",
    "    csv(\"/public/retail_db/orders\")\n",
    "orders.createOrReplaceTempView(\"orders_temp\")\n",
    "```\n",
    "\n",
    "**Using Python**\n",
    "```\n",
    "orders = spark.read. \\\n",
    "    schema(\"order_id INT, order_date STRING, order_customer_id INT, order_status STRING\"). \\\n",
    "    csv(\"/public/retail_db/orders\")\n",
    "orders.createOrReplaceTempView(\"orders_temp\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val orders = spark.read.\n",
    "    schema(\"order_id INT, order_date STRING, order_customer_id INT, order_status STRING\").\n",
    "    csv(\"/public/retail_db/orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders.createOrReplaceTempView(\"orders_temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT o.*, lower(order_status) AS order_status_lower FROM orders_temp AS o LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## String Manipulation Functions\n",
    "\n",
    "We use string manipulation functions quite extensively. Here are some of the important functions which we typically use.\n",
    "\n",
    "* Case Conversion - `lower`, `upper`, `initcap`\n",
    "* Getting size of the column value - `length`\n",
    "* Extracting Data - `substr` and `split`\n",
    "* Trimming and Padding functions - `trim`, `rtrim`, `ltrim`, `rpad` and `lpad`\n",
    "* Reversing strings - `reverse`\n",
    "* Concatenating multiple strings `concat` and `concat_ws`\n",
    "\n",
    "### Case Conversion and Length\n",
    "Let us understand how to perform case conversion functions of a string and also length of a string.\n",
    "\n",
    "* Case Conversion Functions - `lower`, `upper`, `initcap`\n",
    "\n",
    "### Extracting Data - substr and split\n",
    "Let us understand how to extract data from strings using `substr`/`substring` and `split`.\n",
    "\n",
    "* We can get syntax and symantecs of the functions using `DESCRIBE FUNCTION`\n",
    "* We can extract first four characters from string using substr or substring.\n",
    "\n",
    "Let us understand how to extract the information from the string where there is a delimiter.\n",
    "* `split` converts delimited string into array.\n",
    "\n",
    "* We can use explode to convert an array into records.\n",
    "\n",
    "### Trimming and Padding Functions\n",
    "\n",
    "Let us understand how to trim or remove leading and/or trailing spaces in a string.\n",
    "\n",
    "* `ltrim` is used to remove the spaces on the left side of the string.\n",
    "* `rtrim` is used to remove the spaces on the right side of the string.\n",
    "* `trim` is used to remove the spaces on both sides of the string.\n",
    "\n",
    "Let us understand how to use padding to pad characters to a string.\n",
    "\n",
    "* Let us assume that there are 3 fields - year, month and date which are of type integer.\n",
    "* If we have to concatenate all the 3 fields and create a date, we might have to pad month and date with 0.\n",
    "* `lpad` is used more often than `rpad` especially when we try to build the date from separate columns.\n",
    "\n",
    "\n",
    "### Reverse and Concatenating multiple strings\n",
    "\n",
    "Let us understand how to reverse a string as well as concatenate multiple strings.\n",
    "* We can use `reverse` to reverse a string.\n",
    "* We can concatenate multiple strings using `concat` and `concat_ws`.\n",
    "* `concat_ws` is typically used if we want to have the same string between all the strings that are being concatenated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date Manipulation Functions\n",
    "\n",
    "Let us go through some of the important date manipulation functions.\n",
    "\n",
    "* Getting Current Date and Timestamp\n",
    "* Date Arithmetic such as `date_add`\n",
    "* Getting beginning date or time using `trunc` or `date_trunc`\n",
    "* Extracting information using `date_format` as well as calendar functions.\n",
    "* Dealing with unix timestamp using `from_unixtime`, `to_unix_timestamp`\n",
    "\n",
    "### Getting Current Date and Timestamp\n",
    "\n",
    "Let us understand how to get the details about current or today's date as well as current timestamp.\n",
    "\n",
    "* `current_date` is the function or operator which will return today's date.\n",
    "* `current_timestamp` is the function or operator which will return current time up to milliseconds.\n",
    "* These are not like other functions and do not use **()** at the end.\n",
    "* These are not listed as part of `SHOW functions` and we can get help using `DESCRIBE`.\n",
    "* There is a format associated with date and timestamp.\n",
    "  * Date - `yyyy-MM-dd`\n",
    "  * Timestamp - `yyyy-MM-dd HH:mm:ss.SSS`\n",
    "* Keep in mind that a date or timestamp in Spark SQL are nothing but special strings containing values using above specified formats. We can apply all string manipulation functions on date or timestamp.\n",
    "\n",
    "### Date Arithmetic\n",
    "Let us understand how to perform arithmetic on dates or timestamps.\n",
    "\n",
    "* `date_add` can be used to add or subtract days.\n",
    "* `date_sub` can be used to subtract or add days.\n",
    "* `datediff` can be used to get difference between 2 dates\n",
    "* `add_months` can be used add months to a date\n",
    "\n",
    "\n",
    "### Beginning Date or Time - trunc and date_trunc\n",
    "Let us understand how to use `trunc` and `date_trunc` on dates or timestamps and get beginning date of the period.\n",
    "\n",
    "* We can use **MM** to get beginning date of the month.\n",
    "* **YY** can be used to get begining date of the year.\n",
    "* We can apply trunc either on date or timestamp, however we cannot apply it other than month or year (such an hour or day).\n",
    "\n",
    "* While `trunc` can be used to  beginning time of a given month or year, we can get the beginning time up to Second using `date_trunc`.get\n",
    "\n",
    "### Extracting information using date_format\n",
    "\n",
    "Let us understand how to use `date_format` to extract information from date or timestamp.\n",
    "\n",
    "Here is how we can get date related information such as year, month, day etc from date or timestamp.\n",
    "\n",
    "\n",
    "Note: date_format(<column>,'yyyyMM') === to convert column to required datae format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT current_timestamp AS current_timestamp, \n",
    "    date_format(current_timestamp, 'dd') AS day_of_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Here is how we can get the information from date or timestamp in the format we require."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT date_format(current_timestamp, 'yyyyMM') AS current_month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting information - Calendar functions\n",
    "\n",
    "We can get year, month, day etc from date or timestamp using functions.\n",
    " There are functions such as `day`, `dayofmonth`, `month`, `weekofyear`, `year` etc available for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%sql\n",
    "\n",
    "SELECT weekofyear(current_date) AS weekofyear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Unix Timestamp\n",
    "\n",
    "Let us go through the functions that can be used to deal with Unix Timestamp.\n",
    "\n",
    "* `from_unixtime` can be used to convert Unix epoch to regular timestamp.\n",
    "* `unix_timestamp` or `to_unix_timestamp` can be used to convert timestamp to Unix epoch.\n",
    "* We can get Unix epoch or Unix timestamp by running `date '+%s'` in Unix/Linux terminal\n",
    "* We can DESCRIBE on the above functions to get details about them.\n",
    "\n",
    "Let us sww how we can use functions such as `from_unixtime`, `unix_timestamp` or `to_unix_timestamp` to convert between timestamp and Unix timestamp or epoch.\n",
    "\n",
    "* We can unix epoch in Unix/Linux terminal using `date '+%s'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT from_unixtime(1556662731, 'yyyy-MM-dd') AS date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT to_unix_timestamp('20190430 18:18:51', 'yyyyMMdd HH:mm:ss') AS timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Numeric Functions\n",
    "\n",
    "Here are some of the numeric functions we might use quite often.\n",
    "\n",
    "* `abs` - always return positive number\n",
    "* `sum`, `avg`\n",
    "* `round` - rounds off to specified precision\n",
    "* `ceil`, `floor` - always return integer.\n",
    "* `greatest`\n",
    "* `min`, `max`\n",
    "* `rand`\n",
    "* `pow`, `sqrt`\n",
    "* `cumedist`, `stddev`, `variance`\n",
    "\n",
    "Some of the functions highlighted are aggregate functions, eg: `sum`, `avg`, `min`, `max` etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type Conversion\n",
    "\n",
    "Let us understand how we can type cast to change the data type of extracted value to its original type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS orders_single_column (\n",
    "    s STRING\n",
    ") LOCATION '/user/itversity/warehouse/itversity_retail.db/orders'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT split(s, ',')[0] AS order_id,\n",
    "    split(s, ',')[1] AS order_date,\n",
    "    split(s, ',')[2] AS order_customer_id,\n",
    "    split(s, ',')[3] AS order_status\n",
    "FROM orders_single_column LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling NULL Values\n",
    "\n",
    "Let us understand how to handle nulls using specific functions in Spark SQL.\n",
    "\n",
    "* By default if we try to add or concatenate null to another column or expression or literal, it will return null.\n",
    "* If we want to replace null with some default value, we can use `nvl`. For not null values, nvl returns the original expression value.\n",
    "  * Replace commission_pct with 0 if it is null.\n",
    "  * We can also use `coalesce` in the place of `nvl`.\n",
    "  * nvl(<col>,replacement)\n",
    "  * nvl2():If expr1 is not null, then NVL2 returns expr2.\n",
    "* `coalesce` returns first not null value if we pass multiple arguments to it.\n",
    "* `nvl2` can be used to perform one action when the value is not null and some other action when the value is null.\n",
    "  * We want to increase commission_pct by 1 if it is not null and set commission_pct to 2 if it is null.\n",
    "* We can also use `CASE WHEN ELSE END` for any conditional logic.\n",
    "\n",
    "\n",
    "## Using CASE and WHEN\n",
    "At times we might have to select values from multiple columns conditionally.\n",
    "\n",
    "* We can use `CASE` and `WHEN` for that.\n",
    "* Let us implement this conditional logic to come up with derived order_status.\n",
    "  * If order_status is COMPLETE or CLOSED, set COMPLETED\n",
    "  * If order_status have PENDING in it, then we will say PENDING\n",
    "  * If order_status have PROCESSING or PAYMENT_REVIEW in it, then we will say PENDING\n",
    "  * We will set all others as OTHER\n",
    "* We can also have `ELSE` as part of `CASE` and `WHEN`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT DISTINCT order_status,\n",
    "    CASE \n",
    "        WHEN order_status IN ('COMPLETE', 'CLOSED') THEN 'COMPLETED'\n",
    "        WHEN order_status LIKE '%PENDING%' OR order_status IN ('PROCESSING', 'PAYMENT_REVIEW')\n",
    "            THEN 'PENDING'\n",
    "        ELSE 'OTHER'\n",
    "    END AS updated_order_status\n",
    "FROM orders\n",
    "ORDER BY updated_order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Example - Word Count\n",
    "\n",
    "Let us see how we can perform word count using Spark SQL. Using word count as an example we will understand how we can come up with the solution using pre-defined functions available.\n",
    "\n",
    "* Create table by name lines.\n",
    "* Insert data into the table.\n",
    "* Split lines into array of words.\n",
    "* Explode array of words from each line into individual records.\n",
    "* Use group by and get the count. We cannot use `GROUP BY` directly on exploded records and hence we need to use nested sub query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT count(1) FROM (SELECT explode(split(s, ' ')) AS words FROM lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Windowing Functions\n",
    "\n",
    "As part of this section we will primarily talk about Windowing Functions. These are also known as Analytic Functions in Databases like Oracle.\n",
    "\n",
    "* Prepare HR Database\n",
    "* Overview of Windowing Functions\n",
    "* Aggregations using Windowing Functions\n",
    "* Getting LEAD and LAG values\n",
    "* Getting first and last values\n",
    "* Ranking using Windowing Functions\n",
    "* Understanding order of execution of SQL\n",
    "* Overview of Nested Sub Queries\n",
    "* Filtering - Window Function Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare HR Database\n",
    "\n",
    "Let us prepare HR database with **EMPLOYEES** Table. We will be using this for some of the examples as well as exercises related to Window Functions.\n",
    "\n",
    "* Create Database **itversity_hr** (replace itversity with your OS User Name)\n",
    "* Create table **employees** in **itversity_hr** database.\n",
    "* Load data into the table.\n",
    "\n",
    "First let us start with creating the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE employees (\n",
    "  employee_id     int,\n",
    "  first_name      varchar(20),\n",
    "  last_name       varchar(25),\n",
    "  email           varchar(25),\n",
    "  phone_number    varchar(20),\n",
    "  hire_date       date,\n",
    "  job_id          varchar(10),\n",
    "  salary          decimal(8,2),\n",
    "  commission_pct  decimal(2,2),\n",
    "  manager_id      int,\n",
    "  department_id   int\n",
    ") ROW FORMAT DELIMITED FIELDS TERMINATED BY '\\t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "LOAD DATA LOCAL INPATH '/data/hr_db/employees' \n",
    "INTO TABLE employees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Windowing Functions\n",
    "\n",
    "Let us get an overview of Analytics or Windowing Functions in Spark SQL.\n",
    "\n",
    "* Aggregate Functions (`sum`, `min`, `max`, `avg`)\n",
    "* Window Functions (`lead`, `lag`, `first_value`, `last_value`)\n",
    "* Rank Functions (`rank`, `dense_rank`, `row_number` etc)\n",
    "* For all the functions we use `OVER` clause.\n",
    "* For aggregate functions we typically use `PARTITION BY`\n",
    "* For global ranking and windowing functions we can use `ORDER BY sorting_column` and for ranking and windowing with in a partition or group we can use `PARTITION BY partition_column ORDER BY sorting_column`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT employee_id, department_id, salary,\n",
    "    count(1) OVER (PARTITION BY department_id) AS employee_count,\n",
    "    rank() OVER (ORDER BY salary DESC) AS rnk,\n",
    "    lead(employee_id) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_id,\n",
    "    lead(salary) OVER (PARTITION BY department_id ORDER BY salary DESC) AS lead_emp_sal\n",
    "FROM employees\n",
    "ORDER BY employee_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregations using Windowing Functions\n",
    "\n",
    "Let us see how we can perform aggregations with in a partition or group using Windowing/Analytics Functions.\n",
    "\n",
    "* For simple aggregations where we have to get grouping key and aggregated results we can use **GROUP BY**.\n",
    "* If we want to get the raw data along with aggregated results, then using **GROUP BY** is not possible or overly complicated.\n",
    "* Using aggregate functions with **OVER** Clause not only simplifies the process of writing query, but also better with respect to performance.\n",
    "* Let us take an example of getting employee salary percentage when compared to department salary expense.\n",
    "\n",
    "\n",
    "> Let us see how we can get it using Analytics/Windowing Functions. \n",
    "\n",
    "* We can use all standard aggregate functions such as `count`, `sum`, `min`, `max`, `avg` etc.\n",
    "\n",
    "## Using LEAD or LAG\n",
    "\n",
    "Let us understand LEAD and LAG functions to get column values from following or prior records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking using Windowing Functions\n",
    "\n",
    "Let us see how we can assign ranks using different **rank** functions.\n",
    "* If we have to assign ranks globally, we just need to specify **ORDER BY**\n",
    "* If we have to assign ranks with in a key then we need to specify **PARTITION BY** and then **ORDER BY**.\n",
    "* By default **ORDER BY** will sort the data in ascending order. We can change the order by passing **DESC** after order by.\n",
    "* We have 3 main functions to assign ranks - `rank`, `dense_rank` and `row_number`. We will see the difference between the 3 in a moment.\n",
    "\n",
    "Here is an example to assign sparse ranks using the table daily_product_revenue with in each day based on revenue. We can use `rank` function to assign sparse ranks.\n",
    "\n",
    "Let us understand the difference between **rank**, **dense_rank** and **row_number**.\n",
    "\n",
    "* We can use either of the functions to generate ranks when there are no duplicates in the column based on which ranks are assigned.\n",
    "* When the column based on which ranks are assigned have duplicates then row_number should not be used as it generate unique number for each record with in the partition. For those duplicate values, the row number need not be same across multiple runs.\n",
    "* **rank** will skip the ranks in between if multiple people get the same rank while **dense_rank** will not skip the ranks based up on the number of times the value is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order of execution of SQL\n",
    "\n",
    ".\n",
    "\n",
    "1. **FROM**\n",
    "2. **JOIN** or **OUTER JOIN** with **ON**\n",
    "3. **WHERE**\n",
    "4. **GROUP BY** and optionally **HAVING**\n",
    "5. **SELECT**\n",
    "6. **ORDER BY**\n",
    "\n",
    "As **SELECT** is executed before **ORDER BY** clause, we will not be able to refer the aliases defined in **SELECT** caluse in other clauses except for **ORDER BY** in most of the traditional databases. However, in Spark we can specify the aliases defined in **SELECT** in **HAVING** as well as **ORDER BY**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Sub Queries\n",
    "\n",
    "Let us recap about Sub Queries.\n",
    "* We typically have Sub Queries in **FROM** Clause.\n",
    "* We need not provide alias to the Sub Queries in **FROM** Clause in Spark SQL. In earlier versions, you might have to provide alias for the Sub Query.\n",
    "* We use Sub Queries quite often over queries using Analytics/Windowing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering - Window Function Results\n",
    "\n",
    "Let us understand how to filter on top of results of Window Functions.\n",
    "* We can use **Window Functions** only in **SELECT** Clause.\n",
    "* If we have to filter based on Window Function results, then we need to use Sub Queries.\n",
    "* Once the query with window functions is defined as sub query, we can apply filter using aliases provided for the Window Functions.\n",
    "\n",
    "Here is the example where we can filter data based on Window Functions.\n",
    "\n",
    "### Ranking and Filtering - Recap\n",
    "\n",
    "Let us recap the procedure to get top 5 orders by revenue for each day.\n",
    "\n",
    "* We have our original data in **orders** and **order_items**\n",
    "* We can pre-compute the data and store in a table or create a view with the logic to generate **daily product revenue**\n",
    "* Then, we have to use the view or table or even sub query to compute rank\n",
    "* We can use the query with ranks as sub query to filter so that we can get top 5 products by revenue.\n",
    "* Let us see the overall process in action.\n",
    "\n",
    "Let us come up with the query to compute daily product revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cumulative or Moving Aggregations\n",
    "\n",
    "Let us understand how we can take care of cumulative or moving aggregations using Spark SQL.\n",
    "* When it comes to Windowing or Analytic Functions we can also specify window using `ROWS BETWEEN` clause.\n",
    "* We can leverage `ROWS BETWEEN` for cumulative aggregations or moving aggregations.\n",
    "* Here is an example of cumulative sum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### question of moving sum \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS cumulative_daily_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'), \n",
    "    order_date\n",
    "\"\"\").\n",
    "    show(100, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT t.*,\n",
    "    round(sum(t.revenue) OVER (\n",
    "        PARTITION BY date_format(order_date, 'yyyy-MM')\n",
    "        ORDER BY order_date\n",
    "        ROWS BETWEEN 3 PRECEDING AND CURRENT ROW\n",
    "    ), 2) AS moving_3day_revenue\n",
    "FROM daily_revenue t\n",
    "ORDER BY date_format(order_date, 'yyyy-MM'),\n",
    "    order_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c833d68cfa606615f21364fbb488a2420df1f4f5f7a7d6ff6a76f9da5ef2719b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
